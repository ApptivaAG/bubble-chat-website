---
layout: glossary
tags: glossary
title: GPT
summary: Generative Pre-trained Transformer (GPT) sind eine Variante [grosser Sprachmodelle (LLM)](/glossar/grosse-sprachmodelle/) und spielen eine fundamentale Rolle im Bereich der generativen Künstlichen Intelligenz. Das erste GPT-Modell wurde 2018 von [OpenAI](https://openai.com/){target="_blank"} vorgestellt. Diese Modelle basieren auf neuronalen Netzwerken, die mit umfangreichen Mengen an Texten trainiert worden sind und fähig sind, neue menschenähnliche Inhalte zu kreieren. Seit 2023 verfügen die meisten LLMs über diese Eigenschaften und werden manchmal allgemein als GPTs bezeichnet.
eleventyNavigation:
  key: GPT
  parent: Glossar
---

# Was sind Generative Pre-trained Transformers?

Generative Pre-trained Transformer (GPT) sind eine Variante [grosser Sprachmodelle (LLM)](/glossar/grosse-sprachmodelle/) und spielen eine fundamentale Rolle im Bereich der generativen Künstlichen Intelligenz. Das erste GPT-Modell wurde 2018 von [OpenAI](https://openai.com/){target="_blank"} vorgestellt. Diese Modelle basieren auf neuronalen Netzwerken, die mit umfangreichen Mengen an Texten trainiert worden sind und fähig sind, neue menschenähnliche Inhalte zu kreieren. Seit 2023 verfügen die meisten LLMs über diese Eigenschaften und werden manchmal allgemein als GPTs bezeichnet.

## Welche Eigenschaften haben GPTs?

- **Generativ**<br>Sie erzeugen neue Informationen wie zum Beispiel Text, Bilder oder Musik.

- **Vortrainiert**<br>Sie durchlaufen zunächst eine unbeaufsichtigte Vortrainingsphase unter Verwendung eines sehr grossen Datensatzes. Dann durchlaufen sie eine überwachte Feinabstimmungsphase, um das Modell zu steuern. Modelle können auf bestimmte Aufgaben, wie zum Beispiel das Komponieren von Musik, fein abgestimmt werden.

- **Transformatoren**<br>Sie verwenden ein Modell das lernt, Beziehungen von Wörtern herzustellen. Das heisst, es lernt, welches Wort mit hoher Wahrscheinlichkeit in einem Text auf ein anderes Wort folgt.
